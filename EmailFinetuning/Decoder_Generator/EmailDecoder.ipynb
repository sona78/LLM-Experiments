{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Any\n",
        "import pandas as pd\n",
        "import tokenizers\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DVKZNSmRH8Cu"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMS\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_SIZE = 512\n",
        "RNN_LAYERS = 2\n",
        "ATTENTION_HEADS = 4\n",
        "BATCH_SIZE = 128\n",
        "DATA_MAX_LEN = 512\n",
        "EPOCHS = 30\n",
        "DROPOUT = 0.2\n",
        "WEIGHT_DECAY = 0.1\n",
        "TEMPERATURE = 0.7\n",
        "GRAD_CLIP = 1.0\n",
        "LEARNING_RATE = 0.0005"
      ],
      "metadata": {
        "id": "FKaZHnTrL1Rs"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "r5LBQAQ_BpGd"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add positional information to embeddings.\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_size, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_size, num_layers=RNN_LAYERS, batch_first=True)\n",
        "        self.pos_encoding = PositionalEncoding(embed_dim, max_len=DATA_MAX_LEN)\n",
        "        self.attention = nn.MultiheadAttention(hidden_size, dropout=DROPOUT, num_heads=ATTENTION_HEADS, batch_first=True)\n",
        "        self.projection = nn.Linear(hidden_size, vocab_size)\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.pos_encoding(out)\n",
        "        rnn_out, _ = self.rnn(out)\n",
        "        out, _ = self.attention(rnn_out, rnn_out, rnn_out)\n",
        "        out = self.norm1(rnn_out + self.dropout(out))\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EmailDataset(Dataset):\n",
        "    \"\"\"Dataset for email sequences.\"\"\"\n",
        "    def __init__(self, texts, tokenizer, max_length=DATA_MAX_LEN):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer.encode(text).ids\n",
        "        if len(encoding) < self.max_length + 1:\n",
        "            encoding += [0] * (self.max_length + 1 - len(encoding))\n",
        "        else:\n",
        "            encoding = encoding[:self.max_length + 1]\n",
        "        return torch.tensor(encoding[:-1]).long(), torch.tensor(encoding[1:]).long()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_data(path,  batch_size=BATCH_SIZE):\n",
        "    data = pd.read_csv(path).dropna().to_numpy()\n",
        "    data = ['Subject: ' + x[0] + '\\n\\n' + x[1] for x in data]\n",
        "\n",
        "    dataset = EmailDataset(data, tokenizer)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size, shuffle=True, num_workers=4)\n",
        "    return (train_loader, test_loader)"
      ],
      "metadata": {
        "id": "Ve5mofLiFE_l"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(tokenizer, train_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE):\n",
        "    model =  Decoder(embed_dim=EMBED_DIM, hidden_size=HIDDEN_SIZE, vocab_size=tokenizer.get_vocab_size())\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(\n",
        "                outputs.view(-1, tokenizer.get_vocab_size()),\n",
        "                labels.view(-1)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(train_loader)}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZQ5Ru_QtE0pc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, tokenizer):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    model = model.to(device)\n",
        "    total_loss = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]'))\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations during evaluation\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(\n",
        "                outputs.view(-1, tokenizer.get_vocab_size()),\n",
        "                labels.view(-1)\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg_loss}\")\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "DU2SNCbQFkSE"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tokenizers.Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_loader, test_loader = retrieve_data('gmail_data.csv')\n",
        "\n",
        "model = train_model(tokenizer, train_loader)\n",
        "test_model(model, test_loader, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l93FbDf4D5Ne",
        "outputId": "b23829b9-8e47-4c7a-eb18-51baf17f380b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 8.352147531509399\n",
            "Epoch 1, Loss: 6.196687412261963\n",
            "Epoch 2, Loss: 5.512816286087036\n",
            "Epoch 3, Loss: 5.082492828369141\n",
            "Epoch 4, Loss: 4.733634185791016\n",
            "Epoch 5, Loss: 4.453773069381714\n",
            "Epoch 6, Loss: 4.181771469116211\n",
            "Epoch 7, Loss: 4.011816120147705\n",
            "Epoch 8, Loss: 3.885533857345581\n",
            "Epoch 9, Loss: 3.7292094945907595\n",
            "Epoch 10, Loss: 3.527302598953247\n",
            "Epoch 11, Loss: 3.449075222015381\n",
            "Epoch 12, Loss: 3.330120301246643\n",
            "Epoch 13, Loss: 3.1487065076828005\n",
            "Epoch 14, Loss: 3.0914467096328737\n",
            "Epoch 15, Loss: 2.9285510063171385\n",
            "Epoch 16, Loss: 2.772164750099182\n",
            "Epoch 17, Loss: 2.687672734260559\n",
            "Epoch 18, Loss: 2.6541017293930054\n",
            "Epoch 19, Loss: 2.5792191505432127\n",
            "Epoch 20, Loss: 2.479838514328003\n",
            "Epoch 21, Loss: 2.4143373727798463\n",
            "Epoch 22, Loss: 2.3157206535339356\n",
            "Epoch 23, Loss: 2.2793318748474123\n",
            "Epoch 24, Loss: 2.250495719909668\n",
            "Epoch 25, Loss: 2.1116225957870483\n",
            "Epoch 26, Loss: 2.095490598678589\n",
            "Epoch 27, Loss: 2.021770405769348\n",
            "Epoch 28, Loss: 1.9334482550621033\n",
            "Epoch 29, Loss: 1.8810409545898437\n",
            "Test Loss: 2.8739431699117026\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.8739431699117026"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, start_text, max_length=50, device='cpu'):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    input_ids = tokenizer.encode(start_text).ids\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_ids = input_ids.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            outputs = model(input_tensor)\n",
        "            next_token_logits = outputs[0, -1, :] / TEMPERATURE\n",
        "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(next_token_probs, num_samples=1).item()\n",
        "\n",
        "            generated_ids.append(next_token_id)\n",
        "\n",
        "            # If it's a special token like [SEP] or [END], stop generation\n",
        "            if next_token_id == tokenizer.token_to_id('[SEP]') or next_token_id == tokenizer.token_to_id('[END]'):\n",
        "                break\n",
        "\n",
        "            input_tensor = torch.tensor(generated_ids).unsqueeze(0).to(device)\n",
        "\n",
        "    return tokenizer.decode(generated_ids)"
      ],
      "metadata": {
        "id": "h4fvQB7vGAyb"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pt')"
      ],
      "metadata": {
        "id": "R5lmlY4-QK6L"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "scripted_model = torch.jit.script(model)\n",
        "scripted_model.save('model_scripted.pt')"
      ],
      "metadata": {
        "id": "2rjLEn0RQT8Y"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_TEXT = \"Hello my name is kaustubh\"\n",
        "result = generate_text(model, tokenizer, START_TEXT, device=device)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDYRuz9yGoiu",
        "outputId": "e3118c14-ccfc-48d4-a317-168c9f6bb049"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello my name is kaustubh hello mr. donange, i have attached my resume below. i have all attached my resume below along. i would it be possible to get a scheduled zoom meeting. i ' m super interested in interning at wayfair. i just applied\n"
          ]
        }
      ]
    }
  ]
}